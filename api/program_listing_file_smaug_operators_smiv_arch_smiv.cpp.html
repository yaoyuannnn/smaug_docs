

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File smiv.cpp &mdash; SMAUG: Simulating Machine Learning Applications Using gem5-Aladdin</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> SMAUG
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../smaug.html">smaug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.html">smaug.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math.html">smaug.math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor.html">smaug.tensor</a></li>
</ul>
<p class="caption"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SMAUG</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Program Listing for File smiv.cpp</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/program_listing_file_smaug_operators_smiv_arch_smiv.cpp.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="program-listing-for-file-smiv-cpp">
<span id="program-listing-file-smaug-operators-smiv-arch-smiv-cpp"></span><h1>Program Listing for File smiv.cpp<a class="headerlink" href="#program-listing-for-file-smiv-cpp" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_smaug_operators_smiv_arch_smiv.cpp.html#file-smaug-operators-smiv-arch-smiv-cpp"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">smaug/operators/smiv/arch/smiv.cpp</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span> <span class="cpf">&lt;assert.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;math.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;string.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&quot;gem5/m5ops.h&quot;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&quot;nnet_fwd.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/ref/activation_functions.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/ref/pooling.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/ref/zeropad.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/smiv/smiv.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/smiv/params.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;utility/data_layout_conversion.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;utility/profiling.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;utility/utility.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;arch/common.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;arch/interface.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;arch/smiv/common.h&quot;</span><span class="cp"></span>

<span class="cp">#ifdef __cplusplus</span>
<span class="cp">#include</span> <span class="cpf">&quot;mkldnn.hpp&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;arch/nnet_mkl.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/mkl/activation_functions.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/mkl/batch_norm.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/mkl/pooling.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;utility/mkl/utility.h&quot;</span><span class="cp"></span>
<span class="cp">#endif</span>

<span class="cp">#ifdef DMA_MODE</span>
<span class="cp">#include</span> <span class="cpf">&quot;gem5_harness.h&quot;</span><span class="cp"></span>
<span class="cp">#endif</span>

<span class="cp">#if ARCHITECTURE == SMIV</span>

<span class="n">smiv_global</span> <span class="n">g_smiv</span><span class="p">;</span>

<span class="kt">void</span> <span class="nf">init_smiv_global</span><span class="p">(</span><span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Use the same accelerator id for all hardware blocks. This means we will</span>
    <span class="c1">// simulate only ONE datapath instead of multiple, which means that the two</span>
    <span class="c1">// blocks can share the scratchpads (without any infrastructure</span>
    <span class="c1">// changes). The key is that we still trace the functions at the _hw level,</span>
    <span class="c1">// so that Aladdin will exit after simulating each block, and we can return</span>
    <span class="c1">// control to the CPU at the right places.  In contrast, if we used two</span>
    <span class="c1">// different ids, we would have two different datapaths that could not share</span>
    <span class="c1">// data directly.</span>
    <span class="n">g_smiv</span><span class="p">.</span><span class="n">kConvolutionHw</span> <span class="o">=</span> <span class="mh">0x0003</span><span class="p">;</span>
    <span class="n">g_smiv</span><span class="p">.</span><span class="n">kInnerProductHw</span> <span class="o">=</span> <span class="mh">0x0003</span><span class="p">;</span>
    <span class="n">g_smiv</span><span class="p">.</span><span class="n">kReductionHw</span> <span class="o">=</span> <span class="mh">0x0003</span><span class="p">;</span>
    <span class="n">g_smiv</span><span class="p">.</span><span class="n">kBatchNormHw</span> <span class="o">=</span> <span class="mh">0x0003</span><span class="p">;</span>
    <span class="n">g_smiv</span><span class="p">.</span><span class="n">kPoolingHw</span> <span class="o">=</span> <span class="mh">0x0003</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">umem_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">g_smiv</span><span class="p">.</span><span class="n">kUmemSize</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">umem_size</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">g_smiv</span><span class="p">.</span><span class="n">kUmemSize</span> <span class="o">=</span> <span class="n">SMIV_DEFAULT_UMEM_SIZE</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">spad_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">g_smiv</span><span class="p">.</span><span class="n">kSpadSize</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">spad_size</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">g_smiv</span><span class="p">.</span><span class="n">kSpadSize</span> <span class="o">=</span> <span class="n">SMIV_DEFAULT_SPAD_SIZE</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">l2_size</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">g_smiv</span><span class="p">.</span><span class="n">kL2Size</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">l2_size</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">g_smiv</span><span class="p">.</span><span class="n">kL2Size</span> <span class="o">=</span> <span class="n">SMIV_DEFAULT_L2_SIZE</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Size of UMEM: %lu bytes</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">.</span><span class="n">kUmemSize</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Size of Scratchpad: %lu bytes</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">.</span><span class="n">kSpadSize</span><span class="p">);</span>
    <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Size of L2 cache: %lu bytes</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">.</span><span class="n">kL2Size</span><span class="p">);</span>

    <span class="n">g_smiv</span><span class="p">.</span><span class="n">umem</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">kUmemSize</span><span class="p">);</span>
    <span class="n">g_smiv</span><span class="p">.</span><span class="n">spad0</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">kSpadSize</span><span class="p">);</span>
    <span class="n">g_smiv</span><span class="p">.</span><span class="n">spad1</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">kSpadSize</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">free_smiv_global</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">free</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">umem</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">spad0</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">spad1</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">result_buf</span> <span class="nf">flatten_input</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                         <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                         <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                         <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">begin_profiling</span><span class="p">(</span><span class="n">__func__</span><span class="p">,</span> <span class="n">lnum</span><span class="p">);</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">im2row</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">,</span> <span class="n">results</span><span class="p">);</span>
    <span class="n">end_profiling</span><span class="p">();</span>
    <span class="k">return</span> <span class="n">results</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">result_buf</span> <span class="nf">smiv_activation_function</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                                    <span class="n">layer_t</span><span class="o">*</span> <span class="n">layer</span><span class="p">,</span>
                                    <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                                    <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">)</span> <span class="p">{</span>
<span class="cp">#ifdef SMIV_USE_MKL_ACTIVATION_FUNCTION_IMPL</span>
    <span class="c1">// MKL&#39;s implementation requires a separate output buffer.</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
<span class="cp">#endif</span>

    <span class="n">smiv_activation_function_impl</span><span class="p">(</span><span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span>
                                  <span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>

<span class="cp">#ifdef SMIV_USE_MKL_ACTIVATION_FUNCTION_IMPL</span>
    <span class="k">return</span> <span class="n">results</span><span class="p">;</span>
<span class="cp">#else</span>
    <span class="c1">// Our own implementation is in-place.</span>
    <span class="k">return</span> <span class="n">activations</span><span class="p">;</span>
<span class="cp">#endif</span>
<span class="p">}</span>


<span class="n">result_buf</span> <span class="nf">inner_product_layer</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">host_activations</span><span class="p">,</span>
                               <span class="n">data_list</span><span class="o">*</span> <span class="n">host_weights</span><span class="p">,</span>
                               <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                               <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                               <span class="n">data_list</span><span class="o">*</span> <span class="n">host_results</span><span class="p">,</span>
                               <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                               <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">host_results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">host_results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
    <span class="n">smiv_inner_product_layer_impl</span><span class="p">(</span>
            <span class="n">host_activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">host_weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
            <span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">,</span> <span class="n">host_results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">g_smiv</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">host_results</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">result_buf</span> <span class="nf">standard_convolution_layer</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                                      <span class="n">data_list</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span>
                                      <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                                      <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                                      <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                                      <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                                      <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">layer_t</span> <span class="n">curr_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span> <span class="o">&gt;</span> <span class="n">VECTOR_SIZE</span> <span class="o">||</span>
        <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">&gt;</span> <span class="n">VECTOR_SIZE</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span>
                <span class="s">&quot;[ERROR]: In layer %d: SMIV does not support &quot;</span>
                <span class="s">&quot;convolutional weights with rows or cols &gt; %d!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                <span class="n">curr_layer</span><span class="p">.</span><span class="n">num</span><span class="p">,</span> <span class="n">VECTOR_SIZE</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">assert</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span> <span class="o">&lt;=</span> <span class="n">VECTOR_SIZE</span> <span class="o">&amp;&amp;</span>
           <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">&lt;=</span> <span class="n">VECTOR_SIZE</span><span class="p">);</span>
    <span class="n">io_req_t</span> <span class="n">input_req</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">input_req</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="n">weights_var_name</span> <span class="o">=</span>
            <span class="n">input_req</span> <span class="o">==</span> <span class="n">IO_DMA</span> <span class="o">?</span> <span class="s">&quot;host_weights&quot;</span> <span class="o">:</span> <span class="n">input_req</span> <span class="o">==</span> <span class="n">IO_ACP</span>
                                                           <span class="o">?</span> <span class="s">&quot;acp_weights&quot;</span>
                                                           <span class="o">:</span> <span class="s">&quot;cache_weights&quot;</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">weights_size</span> <span class="o">=</span> <span class="n">WEIGHT_BYTES</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">);</span>
    <span class="n">MAP_ARRAY_TO_ACCEL</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">kConvolutionHw</span><span class="p">,</span> <span class="n">weights_var_name</span><span class="p">,</span>
                       <span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">weights_size</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">has_padding</span><span class="p">(</span><span class="o">&amp;</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">pad</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
                <span class="n">results</span> <span class="p">,</span>
                <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">inputs</span><span class="p">),</span>
                <span class="n">Uncompressed</span><span class="p">);</span>
        <span class="n">copy_zeropad</span><span class="p">(</span><span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">,</span>
                     <span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">);</span>
        <span class="n">PRINT_MSG</span><span class="p">(</span><span class="s">&quot;After zeropadding:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG4D</span><span class="p">(</span><span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
                      <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                      <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
                      <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">height</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG4D_V</span><span class="p">(</span><span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                        <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
                        <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">height</span><span class="p">);</span>
        <span class="n">SWAP_PTRS</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">activations</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
    <span class="n">smiv_standard_convolution_layer_impl</span><span class="p">(</span>
            <span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
            <span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
            <span class="n">layers</span><span class="p">,</span>
            <span class="n">lnum</span><span class="p">,</span>
            <span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
            <span class="o">&amp;</span><span class="n">g_smiv</span><span class="p">,</span>
            <span class="n">device</span><span class="p">,</span>
            <span class="n">sampling_param</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">results</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">result_buf</span> <span class="nf">depthwise_convolution_layer</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                                       <span class="n">data_list</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span>
                                       <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                                       <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                                       <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                                       <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                                       <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">io_req_t</span> <span class="n">input_req</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">input_req</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="n">weights_var_name</span> <span class="o">=</span>
            <span class="n">input_req</span> <span class="o">==</span> <span class="n">IO_DMA</span> <span class="o">?</span> <span class="s">&quot;host_weights&quot;</span> <span class="o">:</span> <span class="n">input_req</span> <span class="o">==</span> <span class="n">IO_ACP</span>
                                                           <span class="o">?</span> <span class="s">&quot;acp_weights&quot;</span>
                                                           <span class="o">:</span> <span class="s">&quot;cache_weights&quot;</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">weights_size</span> <span class="o">=</span> <span class="n">WEIGHT_BYTES</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">);</span>
    <span class="n">MAP_ARRAY_TO_ACCEL</span><span class="p">(</span><span class="n">g_smiv</span><span class="p">.</span><span class="n">kConvolutionHw</span><span class="p">,</span> <span class="n">weights_var_name</span><span class="p">,</span>
                       <span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">weights_size</span><span class="p">);</span>
    <span class="n">layer_t</span> <span class="n">curr_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">];</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">has_padding</span><span class="p">(</span><span class="o">&amp;</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">pad</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
                <span class="n">results</span><span class="p">,</span>
                <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">inputs</span><span class="p">),</span>
                <span class="n">Uncompressed</span><span class="p">);</span>
        <span class="n">copy_zeropad</span><span class="p">(</span><span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">,</span>
                     <span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">);</span>
        <span class="n">PRINT_MSG</span><span class="p">(</span><span class="s">&quot;After zeropadding:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG4D</span><span class="p">(</span><span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
                      <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                      <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
                      <span class="n">curr_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">height</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG4D_V</span><span class="p">(</span><span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                        <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
                        <span class="n">curr_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">height</span><span class="p">);</span>
        <span class="n">SWAP_PTRS</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">results</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
    <span class="n">smiv_depthwise_convolution_layer_impl</span><span class="p">(</span>
            <span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span>
            <span class="n">lnum</span><span class="p">,</span> <span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">g_smiv</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>

    <span class="k">return</span> <span class="n">results</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// SMIV currently uses the FC block to implement a GEMM-based 1x1 convolution</span>
<span class="c1">// because the current model of SMIV in Aladdin doesn&#39;t support 1x1 conv on the</span>
<span class="c1">// CONV block.  Eventually we&#39;ll want to use the CNN block since the CNN block</span>
<span class="c1">// outputs results in NCHW format (where as the FC block outputs data in NHWC</span>
<span class="c1">// format).</span>
<span class="n">result_buf</span> <span class="nf">pointwise_convolution_layer</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                                       <span class="n">data_list</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span>
                                       <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                                       <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                                       <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                                       <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                                       <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Allocate memory to store the transformed input.</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">nhwc_inputs</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="n">dims_t</span> <span class="n">nhwc</span> <span class="o">=</span> <span class="n">convert_nchw_to_nhwc_fp32</span><span class="p">(</span><span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
                                            <span class="n">NUM_TEST_CASES</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">inputs</span><span class="p">,</span>
                                            <span class="n">DATA_ALIGNMENT</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">nhwc_inputs</span><span class="p">);</span>

    <span class="c1">// HACK: We need to modify the layer[lnum] descriptor to reflect the fact</span>
    <span class="c1">// that we&#39;re doing a matrix multiply, but these changes can&#39;t be seen</span>
    <span class="c1">// outside of this function. So, back up the current inputs layer.</span>
    <span class="n">layer_t</span> <span class="n">old_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">];</span>
    <span class="c1">// These are the dimensions needed by the FC block routine.</span>
    <span class="n">dims_t</span> <span class="n">fc_dims</span> <span class="o">=</span> <span class="p">{</span> <span class="n">nhwc</span><span class="p">.</span><span class="n">height</span> <span class="o">*</span> <span class="n">nhwc</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span> <span class="n">nhwc</span><span class="p">.</span><span class="n">cols</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nhwc</span><span class="p">.</span><span class="n">align_pad</span> <span class="p">};</span>
    <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">fc_dims</span><span class="p">;</span>

    <span class="c1">// These are the outputs dimensions expected by the FC block.</span>
    <span class="kt">int</span> <span class="n">weights_cols</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span><span class="p">;</span>
    <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span> <span class="o">=</span>
            <span class="p">(</span><span class="n">dims_t</span><span class="p">){</span> <span class="n">fc_dims</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span> <span class="n">weights_cols</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                      <span class="n">calc_padding</span><span class="p">(</span><span class="n">weights_cols</span><span class="p">,</span> <span class="n">DATA_ALIGNMENT</span><span class="p">)</span> <span class="p">};</span>

    <span class="c1">// Allocate new memory to store the results of the FC. The</span>
    <span class="c1">// activations/results buffers are not necessarily big enough to store this</span>
    <span class="c1">// (due to data alignment).</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">nhwc_outputs</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span>
            <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>

    <span class="c1">// Finally, invoke the FC hardware.</span>
    <span class="n">smiv_inner_product_layer_impl</span><span class="p">(</span><span class="n">nhwc_inputs</span><span class="p">,</span> <span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
                                  <span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">,</span> <span class="n">nhwc_outputs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">g_smiv</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>

    <span class="n">PRINT_MSG_V</span><span class="p">(</span><span class="s">&quot;1x1 GEMM results:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="n">PRINT_DEBUG_V</span><span class="p">(</span><span class="n">nhwc_outputs</span><span class="p">,</span> <span class="n">fc_dims</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                  <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
                  <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">);</span>

    <span class="c1">// Reshape the FC results and convert back from NHWC to NCHW.</span>
    <span class="n">dims_t</span> <span class="n">output_dims</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">old_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span><span class="p">,</span>
        <span class="n">old_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">height</span><span class="p">,</span>
        <span class="n">old_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
        <span class="n">calc_padding</span><span class="p">(</span><span class="n">old_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">height</span><span class="p">,</span> <span class="n">DATA_ALIGNMENT</span><span class="p">)</span>
    <span class="p">};</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
    <span class="n">convert_nhwc_to_nchw_fp32</span><span class="p">(</span><span class="n">nhwc_outputs</span><span class="p">,</span> <span class="n">NUM_TEST_CASES</span><span class="p">,</span> <span class="n">output_dims</span><span class="p">,</span>
                              <span class="n">DATA_ALIGNMENT</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">);</span>

    <span class="c1">// Restore the original layer descriptor.</span>
    <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_layer</span><span class="p">;</span>

    <span class="n">free</span><span class="p">(</span><span class="n">nhwc_inputs</span><span class="p">);</span>
    <span class="n">free</span><span class="p">(</span><span class="n">nhwc_outputs</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">results</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Software implementation. SMIV doesn&#39;t accelerate pooling.</span>
<span class="n">result_buf</span> <span class="nf">pooling_layer</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                         <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                         <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                         <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                         <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                         <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">layer_t</span> <span class="n">curr_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">];</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">act_buf</span> <span class="o">=</span> <span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">out_buf</span> <span class="o">=</span> <span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">use_hw_pooling</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">smiv_pooling_layer_impl</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">g_smiv</span><span class="p">,</span> <span class="n">results</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
<span class="cp">#ifdef __cplusplus</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">pool</span> <span class="o">==</span> <span class="n">MAX</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">nnet_mkl</span><span class="o">::</span><span class="n">max_pooling_3d</span><span class="p">(</span>
                    <span class="n">act_buf</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">],</span> <span class="n">out_buf</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">pool</span> <span class="o">==</span> <span class="n">AVG</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">nnet_mkl</span><span class="o">::</span><span class="n">avg_pooling_3d</span><span class="p">(</span>
                    <span class="n">act_buf</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">],</span> <span class="n">out_buf</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">assert</span><span class="p">(</span><span class="nb">false</span> <span class="o">&amp;&amp;</span> <span class="s">&quot;Unsupported pooling layer type!&quot;</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">nnet_mkl</span><span class="o">::</span><span class="n">MklSession</span><span class="o">*</span> <span class="n">session</span> <span class="o">=</span> <span class="n">nnet_mkl</span><span class="o">::</span><span class="n">get_session</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>
        <span class="n">session</span><span class="o">-&gt;</span><span class="n">run_and_clear</span><span class="p">();</span>
<span class="cp">#else</span>
        <span class="c1">// This code should only get run by the tracer.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">pool</span> <span class="o">==</span> <span class="n">MAX</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">max_pooling</span><span class="p">(</span><span class="n">act_buf</span><span class="p">,</span> <span class="n">out_buf</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">]);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="p">.</span><span class="n">pool</span> <span class="o">==</span> <span class="n">AVG</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">avg_pooling</span><span class="p">(</span><span class="n">act_buf</span><span class="p">,</span> <span class="n">out_buf</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">]);</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">assert</span><span class="p">(</span><span class="nb">false</span> <span class="o">&amp;&amp;</span> <span class="s">&quot;Unsupported pooling layer type!&quot;</span><span class="p">);</span>
        <span class="p">}</span>
<span class="cp">#endif</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">results</span><span class="p">;</span>
<span class="p">}</span>

<span class="n">result_buf</span> <span class="nf">batch_norm_layer</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                            <span class="n">data_list</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span>
                            <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                            <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                            <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                            <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                            <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">create_new_data_list_if_necessary</span><span class="p">(</span>
            <span class="n">results</span><span class="p">,</span>
            <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">].</span><span class="n">outputs</span><span class="p">),</span>
            <span class="n">Uncompressed</span><span class="p">);</span>
    <span class="n">smiv_batch_norm_layer_impl</span><span class="p">(</span><span class="n">activations</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
                               <span class="n">weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">,</span>
                               <span class="n">results</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">g_smiv</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">results</span><span class="p">;</span>
<span class="p">}</span>


<span class="n">result_buf</span> <span class="nf">run_layer</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                     <span class="n">data_list</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span>
                     <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                     <span class="kt">int</span> <span class="n">layer_num</span><span class="p">,</span>
                     <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                     <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                     <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">begin_profiling</span><span class="p">(</span><span class="s">&quot;run_layer&quot;</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">);</span>

    <span class="n">begin_profiling</span><span class="p">(</span><span class="s">&quot;layer_dispatcher&quot;</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">);</span>
    <span class="n">result_buf</span> <span class="n">result_loc</span> <span class="o">=</span> <span class="n">layer_dispatcher</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span>
                                             <span class="n">weights</span><span class="p">,</span>
                                             <span class="n">layers</span><span class="p">,</span>
                                             <span class="n">layer_num</span><span class="p">,</span>
                                             <span class="n">results</span><span class="p">,</span>
                                             <span class="n">device</span><span class="p">,</span>
                                             <span class="n">sampling_param</span><span class="p">);</span>
    <span class="n">end_profiling</span><span class="p">();</span>

    <span class="n">activation_type</span> <span class="n">act_func</span> <span class="o">=</span> <span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">activation</span><span class="p">;</span>
    <span class="kt">bool</span> <span class="n">do_activation</span> <span class="o">=</span> <span class="n">act_func</span> <span class="o">!=</span> <span class="n">NO_ACTIVATION</span><span class="p">;</span>
    <span class="kt">bool</span> <span class="n">do_hw_activation</span> <span class="o">=</span>
            <span class="n">device</span><span class="o">-&gt;</span><span class="n">use_hw_activation_func</span> <span class="o">&amp;&amp;</span>
            <span class="n">smiv_is_supported_activation_func</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">type</span><span class="p">,</span> <span class="n">act_func</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">do_activation</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">do_hw_activation</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">result_loc</span> <span class="o">==</span> <span class="n">results</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">SWAP_PTRS</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">results</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">result_loc</span> <span class="o">=</span> <span class="n">smiv_activation_function</span><span class="p">(</span>
                <span class="n">activations</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">],</span> <span class="n">results</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>
        <span class="n">PRINT_MSG</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\n</span><span class="s">activation function</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG4D</span><span class="p">(</span><span class="n">result_loc</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">,</span>
                      <span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">outputs</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                      <span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span>
                              <span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">outputs</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
                      <span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">outputs</span><span class="p">.</span><span class="n">height</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="n">end_profiling</span><span class="p">();</span>
    <span class="n">dump_profiling_log</span><span class="p">();</span>
    <span class="k">return</span> <span class="n">result_loc</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Set the IO required flags for each layer.</span>
<span class="c1">//</span>
<span class="c1">// Since SMIV can share scratchpads between the conv/fc blocks, we only need</span>
<span class="c1">// IO if we need to send data back to the CPU.</span>
<span class="kt">void</span> <span class="nf">set_io_requirements</span><span class="p">(</span><span class="n">network_t</span><span class="o">*</span> <span class="n">network</span><span class="p">,</span>
                         <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                         <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">layer_num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">layer_num</span> <span class="o">&lt;</span> <span class="n">network</span><span class="o">-&gt;</span><span class="n">depth</span><span class="p">;</span> <span class="n">layer_num</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">];</span>

        <span class="c1">// The input layer is easy.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">layer_num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span> <span class="o">=</span> <span class="n">IO_NONE</span><span class="p">;</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">;</span>
            <span class="k">continue</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="c1">// All weights, by default, must be copied, unless the layer is an FC</span>
        <span class="c1">// using compressed weights (in which case the compression HW does the</span>
        <span class="c1">// copy).</span>
        <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">;</span>

        <span class="n">layer_t</span><span class="o">*</span> <span class="n">prev_layer</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
        <span class="n">layer_t</span><span class="o">*</span> <span class="n">next_layer</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">];</span>
<span class="cp">#if DEBUG_LEVEL &gt; 0</span>
        <span class="c1">// When debugging, if we don&#39;t send the results back, we won&#39;t be able</span>
        <span class="c1">// to see what&#39;s happening.</span>
        <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">;</span>
        <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">;</span>
<span class="cp">#else</span>

        <span class="c1">// We only support DMA for hardware batch norm and pooling layers.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">BATCH_NORM</span> <span class="o">||</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">POOLING</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span> <span class="o">=</span> <span class="n">IO_DMA</span><span class="p">;</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">=</span> <span class="n">IO_DMA</span><span class="p">;</span>
            <span class="k">continue</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// First, determine if we need to dma store the output.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">layer_num</span> <span class="o">==</span> <span class="n">network</span><span class="o">-&gt;</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">||</span>
            <span class="c1">// All these activation functions are unsupported.</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">activation</span> <span class="o">==</span> <span class="n">SOFTMAX</span> <span class="o">||</span>
            <span class="c1">// If we disabled HW activation functions but an activation</span>
            <span class="c1">// function is necessary, we need to send back results.</span>
            <span class="p">(</span><span class="o">!</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">use_hw_activation_func</span> <span class="o">&amp;&amp;</span>
             <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">activation</span> <span class="o">!=</span> <span class="n">NO_ACTIVATION</span><span class="p">)</span> <span class="o">||</span>
            <span class="c1">// For now, conv layers also do not support local caching.</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">CONV_STANDARD</span> <span class="o">||</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">CONV_DEPTHWISE</span> <span class="o">||</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">CONV_POINTWISE</span> <span class="o">||</span>
            <span class="c1">// We need to do data layout on the CPU before invoking pooling</span>
            <span class="c1">// block and we don&#39;t support local caching for batch norm right</span>
            <span class="c1">// now.</span>
            <span class="n">next_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">BATCH_NORM</span> <span class="o">||</span> <span class="n">next_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">POOLING</span> <span class="o">||</span>
            <span class="c1">// If the FC block needs work division, we can&#39;t locally cache.</span>
            <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">FC</span> <span class="o">&amp;&amp;</span> <span class="n">next_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">FC</span> <span class="o">&amp;&amp;</span>
             <span class="n">smiv_inner_product_needs_work_division</span><span class="p">(</span>
                     <span class="o">&amp;</span><span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">],</span> <span class="n">g_smiv</span><span class="p">)))</span> <span class="p">{</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">=</span> <span class="n">IO_NONE</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="c1">// We also support one particular case where we only use ACP/CACHE for</span>
        <span class="c1">// results, but weights/activations will still be transferred by DMA.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_activation_func_offload</span> <span class="o">!=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">if</span> <span class="p">((</span><span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_activation_func_offload</span> <span class="o">==</span> <span class="n">IO_ACP</span> <span class="o">||</span>
                 <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_activation_func_offload</span> <span class="o">==</span> <span class="n">IO_CACHE</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
                <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span> <span class="o">==</span> <span class="n">IO_DMA</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_activation_func_offload</span><span class="p">;</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="n">printf</span><span class="p">(</span><span class="s">&quot;[ERROR]: If cpu_activation_func_offload != &quot;</span>
                       <span class="s">&quot;cpu_default_offload, then cpu_default_offload must be &quot;</span>
                       <span class="s">&quot;IO_DMA.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
                <span class="n">assert</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="c1">// We only do flattening on the CPU, so if the current layer needs</span>
        <span class="c1">// flattening, it means the previous layer needs to send resutls</span>
        <span class="c1">// back to the CPU.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_preprocessing</span> <span class="o">==</span> <span class="n">FLATTEN</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// Since batch norm and pooling blocks only support DMA, so except</span>
            <span class="c1">// them here.</span>
            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">prev_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">BATCH_NORM</span><span class="p">)</span> <span class="o">&amp;&amp;</span>
                <span class="o">!</span><span class="p">(</span><span class="n">prev_layer</span><span class="o">-&gt;</span><span class="n">type</span> <span class="o">==</span> <span class="n">POOLING</span><span class="p">))</span>
                <span class="n">prev_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="c1">// If the previous layer doesn&#39;t need to send back results (e.g.,</span>
        <span class="c1">// FC-&gt;FC caching), the current layer needs no IO for inputs.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">prev_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">==</span> <span class="n">IO_NONE</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span> <span class="o">=</span> <span class="n">IO_NONE</span><span class="p">;</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">cpu_default_offload</span><span class="p">;</span>
        <span class="p">}</span>
<span class="cp">#endif</span>
    <span class="p">}</span>

    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">layer_num</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">layer_num</span> <span class="o">&lt;</span> <span class="n">network</span><span class="o">-&gt;</span><span class="n">depth</span><span class="p">;</span> <span class="n">layer_num</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Layer %d: dmaLoad = %d, dmaStore = %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">,</span>
               <span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">input_req</span><span class="p">,</span>
               <span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_num</span><span class="p">].</span><span class="n">output_req</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Runs the forward pass of a neural network.</span>
<span class="c1">//</span>
<span class="c1">// This version loads weights on a per layer basis, and activations are</span>
<span class="c1">// ping-ponged between two buffers, activations and results.</span>
<span class="kt">void</span> <span class="nf">nnet_fwd</span><span class="p">(</span><span class="n">data_list</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
              <span class="n">data_list</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span>
              <span class="n">data_list</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
              <span class="n">network_t</span><span class="o">*</span> <span class="n">network</span><span class="p">,</span>
              <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
              <span class="n">sampling_param_t</span><span class="o">*</span> <span class="n">sampling_param</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">init_smiv_global</span><span class="p">(</span><span class="n">device</span><span class="p">);</span>

    <span class="n">M5_SWITCH_CPU</span><span class="p">();</span>

<span class="cp">#ifdef __cplusplus</span>
    <span class="n">nnet_mkl</span><span class="o">::</span><span class="n">MklSession</span><span class="o">*</span> <span class="n">session</span> <span class="o">=</span> <span class="k">new</span> <span class="n">nnet_mkl</span><span class="o">::</span><span class="n">MklSession</span><span class="p">();</span>
    <span class="n">device</span><span class="o">-&gt;</span><span class="n">session</span> <span class="o">=</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">session</span><span class="p">;</span>
<span class="cp">#endif</span>
    <span class="n">set_io_requirements</span><span class="p">(</span><span class="n">network</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">g_smiv</span><span class="p">);</span>

    <span class="c1">//******************//</span>
    <span class="c1">//   PRIMARY LOOP   //</span>
    <span class="c1">//******************//</span>

    <span class="c1">// We need to ensure that we update the original data_list objects, but</span>
    <span class="c1">// internally a lot of pointer-swapping is done to reduce the number of</span>
    <span class="c1">// memory allocations, so to separate these two worlds, create internal</span>
    <span class="c1">// copies.</span>
    <span class="n">data_list</span><span class="o">*</span> <span class="n">activations_internal</span> <span class="o">=</span> <span class="n">activations</span><span class="p">;</span>
    <span class="n">data_list</span><span class="o">*</span> <span class="n">results_internal</span> <span class="o">=</span> <span class="n">results</span><span class="p">;</span>

    <span class="c1">// Alternate between reading from/writing to activations and results so we</span>
    <span class="c1">// can avoid copying matrices. The initial activations is obviously in</span>
    <span class="c1">// &quot;activations&quot;, so that&#39;s where we start.</span>
    <span class="n">result_buf</span> <span class="n">result_loc</span> <span class="o">=</span> <span class="n">activations</span><span class="p">;</span>
    <span class="nl">nnet_fwd_outer</span><span class="p">:</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">l</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">l</span> <span class="o">&lt;</span> <span class="n">network</span><span class="o">-&gt;</span><span class="n">depth</span><span class="p">;</span> <span class="n">l</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">result_loc</span> <span class="o">==</span> <span class="n">results_internal</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">SWAP_PTRS</span><span class="p">(</span><span class="n">results_internal</span><span class="p">,</span> <span class="n">activations_internal</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="n">result_loc</span> <span class="o">=</span> <span class="n">run_layer</span><span class="p">(</span>
                <span class="n">activations_internal</span><span class="p">,</span> <span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">l</span><span class="p">].</span><span class="n">host_weights</span><span class="p">,</span>
                <span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">results_internal</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">sampling_param</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">copy_data_list</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">result_loc</span><span class="p">);</span>
    <span class="n">network</span><span class="o">-&gt;</span><span class="n">layers</span><span class="p">[</span><span class="n">network</span><span class="o">-&gt;</span><span class="n">depth</span> <span class="o">-</span> <span class="mi">1</span><span class="p">].</span><span class="n">result_in_temp</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>

    <span class="n">free_smiv_global</span><span class="p">();</span>
<span class="p">}</span>

<span class="cp">#endif</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, SMAUG Contributors

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>