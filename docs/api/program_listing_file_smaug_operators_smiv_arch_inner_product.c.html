

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Program Listing for File inner_product.c &mdash; SMAUG: Simulating Machine Learning Applications Using gem5-Aladdin</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/collapsible-lists/css/tree_view.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js"></script>
        <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> SMAUG
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../smaug.html">smaug</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn.html">smaug.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../math.html">smaug.math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensor.html">smaug.tensor</a></li>
</ul>
<p class="caption"><span class="caption-text">C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="library_root.html">Library API</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">SMAUG</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Program Listing for File inner_product.c</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/program_listing_file_smaug_operators_smiv_arch_inner_product.c.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="program-listing-for-file-inner-product-c">
<span id="program-listing-file-smaug-operators-smiv-arch-inner-product-c"></span><h1>Program Listing for File inner_product.c<a class="headerlink" href="#program-listing-for-file-inner-product-c" title="Permalink to this headline">¶</a></h1>
<p>↰ <a class="reference internal" href="file_smaug_operators_smiv_arch_inner_product.c.html#file-smaug-operators-smiv-arch-inner-product-c"><span class="std std-ref">Return to documentation for file</span></a> (<code class="docutils literal notranslate"><span class="pre">smaug/operators/smiv/arch/inner_product.c</span></code>)</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span> <span class="cpf">&lt;assert.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;string.h&gt;</span><span class="cp"></span>

<span class="cp">#include</span> <span class="cpf">&quot;arch/common.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;arch/smiv/common.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;arch/smiv/dispatch_utils.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/nnet_fwd_defs.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/smiv/params.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/smiv/smiv.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;core/smiv/smiv.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;utility/compression.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;utility/utility.h&quot;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&quot;config.h&quot;</span><span class="cp"></span>

<span class="cp">#ifdef DMA_MODE</span>
<span class="cp">#include</span> <span class="cpf">&quot;gem5_harness.h&quot;</span><span class="cp"></span>
<span class="cp">#endif</span>

<span class="k">typedef</span> <span class="nf">void</span> <span class="p">(</span><span class="o">*</span><span class="n">tiled_inner_product_impl</span><span class="p">)(</span>
        <span class="kt">float</span><span class="o">*</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span><span class="p">,</span> <span class="n">layer_t</span><span class="o">*</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span><span class="p">,</span> <span class="n">smiv_global</span><span class="o">*</span><span class="p">,</span> <span class="n">device_t</span><span class="o">*</span><span class="p">,</span> <span class="kt">bool</span><span class="p">);</span>

<span class="k">typedef</span> <span class="k">struct</span> <span class="n">_inner_product_options</span> <span class="p">{</span>
    <span class="kt">bool</span> <span class="n">do_bias</span><span class="p">;</span>
    <span class="kt">bool</span> <span class="n">input_in_spad0</span><span class="p">;</span>
    <span class="kt">bool</span> <span class="n">use_pipelined_dma</span><span class="p">;</span>
<span class="p">}</span> <span class="n">smiv_inner_product_options</span><span class="p">;</span>

<span class="c1">// Main implementation of inner product HW.</span>
<span class="c1">//</span>
<span class="c1">// This function handles DMA operations to load the local memory if required,</span>
<span class="c1">// runs the matrix multiply, and then handles the DMA transfer back to the host</span>
<span class="c1">// if required. In the context of this function, &quot;local memory&quot; is any type of</span>
<span class="c1">// memory that the accelerator can access directly (i.e. no DMA required). In</span>
<span class="c1">// other words, &quot;local_activations&quot; could be pointing to SPAD0, but it could</span>
<span class="c1">// have also been &quot;acp_activations&quot;.</span>
<span class="c1">//</span>
<span class="c1">// For this reason, the first three arguments are prefixed with dma_, instead</span>
<span class="c1">// of a more generic prefix (since this wrapper is called with varying</span>
<span class="c1">// arguments), because they are only used if DMA is required.</span>
<span class="c1">//</span>
<span class="c1">// Arguments:</span>
<span class="c1">//   dma_activations: The host address of the input activations.</span>
<span class="c1">//   dma_weights: The host address of the input weights.</span>
<span class="c1">//   dma_results: The host address of the input results.</span>
<span class="c1">//   local_activations: Pointer to inputs that the accelerator reads directly.</span>
<span class="c1">//   local_weights: Pointer to weights that the accelerator reads directly.</span>
<span class="c1">//   local_results: Pointer to results that the accelerator writes directly.</span>
<span class="c1">//   curr_layer: Description of this layer&#39;s shape and parameters.</span>
<span class="c1">//   options: Additional options for this execution of inner product.</span>
<span class="k">static</span> <span class="kt">void</span> <span class="nf">inner_product_layer_hw_impl</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">dma_activations</span><span class="p">,</span>
                                        <span class="kt">float</span><span class="o">*</span> <span class="n">dma_weights</span><span class="p">,</span>
                                        <span class="kt">float</span><span class="o">*</span> <span class="n">dma_results</span><span class="p">,</span>
                                        <span class="kt">float</span><span class="o">*</span> <span class="n">local_activations</span><span class="p">,</span>
                                        <span class="kt">float</span><span class="o">*</span> <span class="n">local_weights</span><span class="p">,</span>
                                        <span class="kt">float</span><span class="o">*</span> <span class="n">local_results</span><span class="p">,</span>
                                        <span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                        <span class="n">smiv_inner_product_options</span><span class="o">*</span> <span class="n">options</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights_req</span> <span class="o">==</span> <span class="n">IO_DMA</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ASSERT</span><span class="p">(</span><span class="n">dma_weights</span> <span class="o">&amp;&amp;</span> <span class="s">&quot;DMA weights pointer cannot be NULL!&quot;</span><span class="p">);</span>
        <span class="kt">int</span> <span class="n">weights_size</span> <span class="o">=</span> <span class="n">get_num_weights_layer</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">options</span><span class="o">-&gt;</span><span class="n">do_bias</span><span class="p">)</span>
            <span class="n">weights_size</span> <span class="o">-=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span><span class="p">;</span>
        <span class="n">weights_size</span> <span class="o">*=</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
        <span class="n">setReadyBits</span><span class="p">(</span><span class="n">local_weights</span><span class="p">,</span> <span class="n">weights_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
        <span class="n">dma_load_wrapper</span><span class="p">(</span><span class="n">local_weights</span><span class="p">,</span> <span class="n">dma_weights</span><span class="p">,</span> <span class="n">weights_size</span><span class="p">,</span>
                         <span class="n">options</span><span class="o">-&gt;</span><span class="n">use_pipelined_dma</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span> <span class="o">==</span> <span class="n">IO_DMA</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ASSERT</span><span class="p">(</span><span class="n">dma_activations</span> <span class="o">&amp;&amp;</span> <span class="s">&quot;DMA inputs pointer cannot be NULL!&quot;</span><span class="p">);</span>
        <span class="kt">int</span> <span class="n">activations_size</span> <span class="o">=</span>
                <span class="n">get_input_activations_size</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
        <span class="n">setReadyBits</span><span class="p">(</span><span class="n">local_activations</span><span class="p">,</span> <span class="n">activations_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
        <span class="n">dma_load_wrapper</span><span class="p">(</span><span class="n">local_activations</span><span class="p">,</span> <span class="n">dma_activations</span><span class="p">,</span> <span class="n">activations_size</span><span class="p">,</span>
                         <span class="n">options</span><span class="o">-&gt;</span><span class="n">use_pipelined_dma</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">matrix_multiply_with_bias_smiv</span><span class="p">(</span>
            <span class="n">local_activations</span><span class="p">,</span>
            <span class="n">local_weights</span><span class="p">,</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span> <span class="n">NUM_TEST_CASES</span><span class="p">,</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">.</span><span class="n">align_pad</span><span class="p">,</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">options</span><span class="o">-&gt;</span><span class="n">do_bias</span><span class="p">,</span>
            <span class="n">local_results</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">output_req</span> <span class="o">==</span> <span class="n">IO_DMA</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">ASSERT</span><span class="p">(</span><span class="n">dma_results</span> <span class="o">&amp;&amp;</span> <span class="s">&quot;DMA results pointer cannot be NULL!&quot;</span><span class="p">);</span>
        <span class="kt">size_t</span> <span class="n">result_size</span> <span class="o">=</span>
                <span class="n">get_output_activations_size</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
        <span class="n">dma_store_wrapper</span><span class="p">(</span><span class="n">dma_results</span><span class="p">,</span> <span class="n">local_results</span><span class="p">,</span> <span class="n">result_size</span><span class="p">,</span>
                          <span class="n">options</span><span class="o">-&gt;</span><span class="n">use_pipelined_dma</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">static</span> <span class="kt">void</span> <span class="nf">inner_product_layer_hw</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">dma_activations</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">dma_weights</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">dma_results</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">cache_activations</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">cache_weights</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">cache_results</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">acp_activations</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">acp_weights</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">acp_results</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">umem</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">spad0</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">spad1</span><span class="p">,</span>
                                   <span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                   <span class="n">access_config</span><span class="o">*</span> <span class="n">access_config</span><span class="p">,</span>
                                   <span class="n">smiv_inner_product_options</span><span class="o">*</span> <span class="n">options</span><span class="p">)</span> <span class="p">{</span>

<span class="c1">//=--------- Convenience macros for invoking the HW impl ---------------=//</span>
<span class="c1">//</span>
<span class="c1">// Each of these macros will call inner_product_layer_hw_impl() with a</span>
<span class="c1">// different name for the array arguments, based on the desired access</span>
<span class="c1">// mechanism for each. Since we name our variables in a consistent way -</span>
<span class="c1">// &quot;mechanism_arrayname&quot; - the macro can automatically form the correct</span>
<span class="c1">// variable name by macro concatentation.</span>
<span class="c1">//</span>
<span class="c1">// If DEBUG_LEVEL &gt;= 2, then each invocation of these macros will print</span>
<span class="c1">// the mechanism and variable names used in the function call.</span>
<span class="c1">//</span>
<span class="c1">// Common argument abbreviations:</span>
<span class="c1">//    HA = host activations</span>
<span class="c1">//    HW = host weights</span>
<span class="c1">//    HR = host results</span>
<span class="c1">//    LA = local activations</span>
<span class="c1">//    LW = local weights</span>
<span class="c1">//    LR = local result</span>

<span class="c1">// No DMA involved, so we can pass NULL pointers for the first three arguments.</span>
<span class="c1">// All args to this macro are mechanism prefixes (e.g. dma, acp, cache).</span>
<span class="cp">#define INNER_PROD_NO_DMA_IMPL(INPUT, WGT, LR)                                 \</span>
<span class="cp">    do {                                                                       \</span>
<span class="cp">        PRINT_MSG(#INPUT &quot;-&quot; #WGT &quot;-&quot; #LR &quot;\n&quot;);                               \</span>
<span class="cp">        inner_product_layer_hw_impl(NULL, NULL, NULL, INPUT##_activations,     \</span>
<span class="cp">                                    WGT##_weights, LR##_results, curr_layer,   \</span>
<span class="cp">                                    options);                                  \</span>
<span class="cp">    } while (0)</span>

<span class="c1">// DMA potentially used for all host arguments. The first three arguments are</span>
<span class="c1">// mechanism prefixes; all other arguments are the full variable names.</span>
<span class="cp">#define INNER_PROD_WITH_DMA_IMPL(HA, HW, HR, LA, LW, LR)                       \</span>
<span class="cp">    do {                                                                       \</span>
<span class="cp">        PRINT_MSG(#HA &quot;-&quot; #HW &quot;-&quot; #HR &quot;-&quot; #LA &quot;-&quot; #LW &quot;-&quot; #LR &quot;\n&quot;);           \</span>
<span class="cp">        inner_product_layer_hw_impl(HA##_activations, HW##_weights,            \</span>
<span class="cp">                                    HR##_results, LA, LW, LR, curr_layer,      \</span>
<span class="cp">                                    options);                                  \</span>
<span class="cp">    } while (0)</span>

<span class="c1">// DMA used, with the input coming from either SPAD0 or SPAD1, but the output</span>
<span class="c1">// is going to a non-scratchpad location. Select the right input array with</span>
<span class="c1">// SELECT_SPAD0.</span>
<span class="cp">#define INNER_PROD_WITH_DMA_SPAD_INPUT_IMPL(                                   \</span>
<span class="cp">        HA, HW, HR, SPAD0, SPAD1, SELECT_SPAD0, LW, LR)                        \</span>
<span class="cp">    do {                                                                       \</span>
<span class="cp">        if (SELECT_SPAD0) {                                                    \</span>
<span class="cp">            INNER_PROD_WITH_DMA_IMPL(HA, HW, HR, SPAD0, LW, LR);               \</span>
<span class="cp">        } else {                                                               \</span>
<span class="cp">            INNER_PROD_WITH_DMA_IMPL(HA, HW, HR, SPAD1, LW, LR);               \</span>
<span class="cp">        }                                                                      \</span>
<span class="cp">    } while (0)</span>

<span class="c1">// DMA used, with the output going to either SPAD0 or SPAD1, but the input is</span>
<span class="c1">// coming from a non-scratchpad location. Select the right one with</span>
<span class="c1">// SELECT_SPAD0.</span>
<span class="cp">#define INNER_PROD_WITH_DMA_SPAD_OUTPUT_IMPL(                                  \</span>
<span class="cp">        HA, HW, HR, LA, LW, SPAD0, SPAD1, SELECT_SPAD0)                        \</span>
<span class="cp">    do {                                                                       \</span>
<span class="cp">        if (SELECT_SPAD0) {                                                    \</span>
<span class="cp">            INNER_PROD_WITH_DMA_IMPL(HA, HW, HR, LA, LW, SPAD1);               \</span>
<span class="cp">        } else {                                                               \</span>
<span class="cp">            INNER_PROD_WITH_DMA_IMPL(HA, HW, HR, LA, LW, SPAD0);               \</span>
<span class="cp">        }                                                                      \</span>
<span class="cp">    } while (0)</span>

<span class="c1">// DMA used, with both inputs and outputs going to/from scratchpads.</span>
<span class="cp">#define INNER_PROD_WITH_DMA_SPAD_IO_IMPL(                                      \</span>
<span class="cp">        HA, HW, HR, LW, SPAD0, SPAD1, SELECT_SPAD0)                            \</span>
<span class="cp">    do {                                                                       \</span>
<span class="cp">        if (SELECT_SPAD0) {                                                    \</span>
<span class="cp">            INNER_PROD_WITH_DMA_IMPL(HA, HW, HR, SPAD0, LW, SPAD1);            \</span>
<span class="cp">        } else {                                                               \</span>
<span class="cp">            INNER_PROD_WITH_DMA_IMPL(HA, HW, HR, SPAD1, LW, SPAD0);            \</span>
<span class="cp">        }                                                                      \</span>
<span class="cp">    } while (0)</span>

    <span class="kt">bool</span> <span class="n">input_in_spad0</span> <span class="o">=</span> <span class="n">options</span><span class="o">-&gt;</span><span class="n">input_in_spad0</span><span class="p">;</span>
    <span class="c1">// These selections use the same mechanism all across.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_IO_IMPL</span><span class="p">(</span>
            <span class="n">dma</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">umem</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span> <span class="n">input_in_spad0</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_NO_DMA_IMPL</span><span class="p">(</span><span class="n">acp</span><span class="p">,</span> <span class="n">acp</span><span class="p">,</span> <span class="n">acp</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_NO_DMA_IMPL</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">// These selections only use _ACP or _Cache for the results.</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_INPUT_IMPL</span><span class="p">(</span>
                <span class="n">dma</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">acp</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span> <span class="n">input_in_spad0</span><span class="p">,</span> <span class="n">umem</span><span class="p">,</span> <span class="n">acp_results</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_INPUT_IMPL</span><span class="p">(</span>
                <span class="n">dma</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span> <span class="n">input_in_spad0</span><span class="p">,</span> <span class="n">umem</span><span class="p">,</span> <span class="n">cache_results</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">// These selections use DMA/None for the inputs.</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_INPUT_IMPL</span><span class="p">(</span><span class="n">dma</span><span class="p">,</span> <span class="n">acp</span><span class="p">,</span> <span class="n">acp</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span>
                                            <span class="n">input_in_spad0</span><span class="p">,</span> <span class="n">acp_weights</span><span class="p">,</span>
                                            <span class="n">acp_results</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_INPUT_IMPL</span><span class="p">(</span><span class="n">dma</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span>
                                            <span class="n">input_in_spad0</span><span class="p">,</span> <span class="n">cache_weights</span><span class="p">,</span>
                                            <span class="n">cache_results</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">// These selections use DMA/None for the inputs/outputs.</span>
    <span class="c1">//</span>
    <span class="c1">// NOTE: This scenario is currently not possible to specify via the model</span>
    <span class="c1">// configuration file.</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_IO_IMPL</span><span class="p">(</span>
            <span class="n">dma</span><span class="p">,</span> <span class="n">acp</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">acp_weights</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span> <span class="n">input_in_spad0</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_IO_IMPL</span><span class="p">(</span>
            <span class="n">dma</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">cache_weights</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span> <span class="n">input_in_spad0</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">// These selections use DMA/None for the outputs.</span>
    <span class="c1">//</span>
    <span class="c1">// TODO: Since we&#39;re not reading out of the scratchpads for the input, it</span>
    <span class="c1">// shouldn&#39;t matter which scratchpad we write into, but currently, the</span>
    <span class="c1">// inner product layer will automatically toggle input_in_spad0 back and</span>
    <span class="c1">// forth regardless of what actually happened. To ensure that future layers</span>
    <span class="c1">// will get the right data, we still have to obey this condition. This</span>
    <span class="c1">// needs to be fixed.</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_OUTPUT_IMPL</span><span class="p">(</span><span class="n">acp</span><span class="p">,</span> <span class="n">acp</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">acp_activations</span><span class="p">,</span>
                                             <span class="n">acp_weights</span><span class="p">,</span> <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span>
                                             <span class="n">input_in_spad0</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_SPAD_OUTPUT_IMPL</span><span class="p">(</span><span class="n">cache</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span>
                                             <span class="n">cache_activations</span><span class="p">,</span> <span class="n">cache_weights</span><span class="p">,</span>
                                             <span class="n">spad0</span><span class="p">,</span> <span class="n">spad1</span><span class="p">,</span> <span class="n">input_in_spad0</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">// These selections only use DMA for the weights.</span>
    <span class="c1">// This is used if the FC layer is using decompressed CSR weights (after</span>
    <span class="c1">// decompression the weights are already in the UMEM so no more data</span>
    <span class="c1">// movement is required).</span>
    <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_ACP</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_IMPL</span><span class="p">(</span>
                <span class="n">acp</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">acp</span><span class="p">,</span> <span class="n">acp_activations</span><span class="p">,</span> <span class="n">umem</span><span class="p">,</span> <span class="n">acp_results</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">DISPATCH_3</span><span class="p">(</span><span class="n">access_config</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">,</span> <span class="n">_DmaOrLocal</span><span class="p">,</span> <span class="n">_Cache</span><span class="p">))</span> <span class="p">{</span>
        <span class="n">INNER_PROD_WITH_DMA_IMPL</span><span class="p">(</span>
                <span class="n">cache</span><span class="p">,</span> <span class="n">dma</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">cache_activations</span><span class="p">,</span> <span class="n">umem</span><span class="p">,</span> <span class="n">cache_results</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="c1">// Otherwise, give up.</span>
    <span class="k">else</span> <span class="p">{</span>
        <span class="n">assert</span><span class="p">(</span><span class="nb">false</span> <span class="o">&amp;&amp;</span>
               <span class="s">&quot;This is an unsupported combination of access mechanisms!&quot;</span><span class="p">);</span>
    <span class="p">}</span>

<span class="cp">#undef INNER_PROD_WITH_DMA_SPAD_INPUT_IMPL</span>
<span class="cp">#undef INNER_PROD_WITH_DMA_SPAD_OUTPUT_IMPL</span>
<span class="cp">#undef INNER_PROD_WITH_DMA_SPAD_IO_IMPL</span>
<span class="cp">#undef INNER_PROD_WITH_DMA_IMPL</span>
<span class="cp">#undef INNER_PROD_NO_DMA_IMPL</span>
<span class="p">}</span>

<span class="c1">// Returns true if this inner product layer will require multiple iterations.</span>
<span class="kt">bool</span> <span class="nf">smiv_inner_product_needs_work_division</span><span class="p">(</span><span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                            <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">total_weight_bytes</span> <span class="o">=</span> <span class="n">WEIGHT_BYTES</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">return</span> <span class="n">total_weight_bytes</span> <span class="o">&gt;</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kUmemSize</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// These are the conditions under which we just will not try to run the layer</span>
<span class="c1">// at all.</span>
<span class="c1">//</span>
<span class="c1">// TODO: These are not quite the right constraints.</span>
<span class="kt">void</span> <span class="nf">smiv_inner_product_check_absolute_size_limits</span><span class="p">(</span><span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                                   <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">total_input_bytes</span> <span class="o">=</span>
            <span class="n">get_input_activations_size</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">)</span> <span class="o">/</span> <span class="n">NUM_TEST_CASES</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">total_input_bytes</span> <span class="o">&gt;</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kSpadSize</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;A single input does not fit in the SPAD, which is not &quot;</span>
               <span class="s">&quot;supported!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">assert</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">total_output_bytes</span> <span class="o">=</span>
            <span class="n">get_output_activations_size</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">)</span> <span class="o">/</span> <span class="n">NUM_TEST_CASES</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">total_output_bytes</span> <span class="o">&gt;</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kSpadSize</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;A single output does not fit in the SPAD, which is not &quot;</span>
               <span class="s">&quot;supported!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">assert</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c1">// Divides the work for a FC layer into several iterations on SMIV.</span>
<span class="c1">//</span>
<span class="c1">// Work division is required when the number of weights exceeds what can be fit</span>
<span class="c1">// on the UMEM. The weight matrix is In x On, where In is the number of input</span>
<span class="c1">// neurons and On is the number of output neurons.</span>
<span class="c1">//</span>
<span class="c1">// Columnwise work division means to do the matrix multiply in groups of In x W,</span>
<span class="c1">// where W = On/iterations. This will require weights reordering, but not input</span>
<span class="c1">// reordering.</span>
<span class="n">fc_cfg_t</span> <span class="nf">smiv_inner_product_divide_work_colwise</span><span class="p">(</span><span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                                <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fc_cfg_t</span> <span class="n">fc_cfgs</span><span class="p">;</span>
    <span class="n">smiv_inner_product_check_absolute_size_limits</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">smiv_inner_product_needs_work_division</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">))</span> <span class="p">{</span>
        <span class="c1">// No work division means to return an fc_cfg_t that is holds the</span>
        <span class="c1">// entire weights.</span>
        <span class="n">init_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
        <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">;</span>
        <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">rows</span> <span class="o">+=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">biases</span><span class="p">.</span><span class="n">rows</span><span class="p">;</span>
        <span class="k">return</span> <span class="n">fc_cfgs</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">// Divide up the weights. The minimum required work (for now) is an Nx8</span>
    <span class="c1">// strip of weights, where N is the number of hidden neurons.</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_inputs</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">biases</span><span class="p">.</span><span class="n">rows</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">num_neurons</span> <span class="o">=</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">minimum_work_size</span> <span class="o">=</span> <span class="n">num_inputs</span> <span class="o">*</span> <span class="n">VECTOR_SIZE</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">minimum_work_size</span> <span class="o">&gt;</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kUmemSize</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;This weights layer exceeds our current capability to run!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">assert</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">max_work_units_per_iteration</span> <span class="o">=</span>
            <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kUmemSize</span> <span class="o">/</span> <span class="n">minimum_work_size</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">bytes_per_iteration</span> <span class="o">=</span>
            <span class="n">max_work_units_per_iteration</span> <span class="o">*</span> <span class="n">minimum_work_size</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">num_cols_per_iteration</span> <span class="o">=</span>
            <span class="n">bytes_per_iteration</span> <span class="o">/</span> <span class="n">num_inputs</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">total_weight_bytes</span> <span class="o">=</span> <span class="n">WEIGHT_BYTES</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">num_iterations</span> <span class="o">=</span>
            <span class="n">ceil</span><span class="p">(((</span><span class="kt">float</span><span class="p">)</span><span class="n">total_weight_bytes</span><span class="p">)</span> <span class="o">/</span> <span class="n">bytes_per_iteration</span><span class="p">);</span>

    <span class="n">init_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">);</span>
    <span class="kt">unsigned</span> <span class="n">num_cols_remaining</span> <span class="o">=</span> <span class="n">num_neurons</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_iterations</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">num_cols_this_iter</span> <span class="o">=</span>
                <span class="n">min2</span><span class="p">(</span><span class="n">num_cols_remaining</span><span class="p">,</span> <span class="n">num_cols_per_iteration</span><span class="p">);</span>
        <span class="c1">// We can ignore align_pad here because num_neurons has already</span>
        <span class="c1">// accounted for the original required padding.</span>
        <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">dims_t</span><span class="p">){</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_cols_this_iter</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span> <span class="p">};</span>
        <span class="n">num_cols_remaining</span> <span class="o">-=</span> <span class="n">num_cols_this_iter</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">fc_cfgs</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Divides the work for a FC layer into several iterations on SMIV.</span>
<span class="c1">//</span>
<span class="c1">// Work division is required when the number of weights exceeds what can be fit</span>
<span class="c1">// on the UMEM. The weight matrix is In x On, where In is the number of input</span>
<span class="c1">// neurons and On is the number of output neurons.</span>
<span class="c1">//</span>
<span class="c1">// Rowwise work division means to do the matrix multiply in groups of W x On,</span>
<span class="c1">// where W = In/iterations. This will require inputs reordering, but not</span>
<span class="c1">// weights reordering.</span>
<span class="n">fc_cfg_t</span> <span class="nf">smiv_inner_product_divide_work_rowwise</span><span class="p">(</span><span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                                <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">fc_cfg_t</span> <span class="n">fc_cfgs</span><span class="p">;</span>
    <span class="n">smiv_inner_product_check_absolute_size_limits</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">smiv_inner_product_needs_work_division</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">))</span> <span class="p">{</span>
        <span class="c1">// No work division means to return an fc_cfg_t that is holds the</span>
        <span class="c1">// entire weights.</span>
        <span class="n">init_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
        <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">;</span>
        <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">rows</span> <span class="o">+=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">biases</span><span class="p">.</span><span class="n">rows</span><span class="p">;</span>
        <span class="k">return</span> <span class="n">fc_cfgs</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">// Divide up the weights. The minimum amount of work is 2xN, where N is the</span>
    <span class="c1">// number of output neurons. Also keep the bias in mind - it should be</span>
    <span class="c1">// omitted until the very last iteration.</span>

    <span class="c1">// num_inputs includes the extra row of biases. If the final iteration has</span>
    <span class="c1">// weights.rows == 1, then we know we should just add the biases in</span>
    <span class="c1">// software; otherwise we do it in HW.</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_inputs</span> <span class="o">=</span>
            <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">biases</span><span class="p">.</span><span class="n">rows</span><span class="p">)</span> <span class="o">*</span>
            <span class="n">NUM_TEST_CASES</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">num_neurons</span> <span class="o">=</span>
            <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">minimum_work_size</span> <span class="o">=</span> <span class="n">num_neurons</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">minimum_work_size</span> <span class="o">&gt;</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kUmemSize</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;This weights layer exceeds our current capability to run!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">assert</span><span class="p">(</span><span class="nb">false</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">max_work_units_per_iteration</span> <span class="o">=</span>
            <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kUmemSize</span> <span class="o">/</span> <span class="n">minimum_work_size</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">bytes_per_iteration</span> <span class="o">=</span>
            <span class="n">max_work_units_per_iteration</span> <span class="o">*</span> <span class="n">minimum_work_size</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">num_rows_per_iteration</span> <span class="o">=</span>
            <span class="n">bytes_per_iteration</span> <span class="o">/</span> <span class="n">num_neurons</span> <span class="o">/</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">total_weight_bytes</span> <span class="o">=</span> <span class="n">WEIGHT_BYTES</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
    <span class="k">const</span> <span class="kt">unsigned</span> <span class="n">num_iterations</span> <span class="o">=</span>
            <span class="n">ceil</span><span class="p">(((</span><span class="kt">float</span><span class="p">)</span><span class="n">total_weight_bytes</span><span class="p">)</span> <span class="o">/</span> <span class="n">bytes_per_iteration</span><span class="p">);</span>

    <span class="n">init_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">);</span>
    <span class="kt">unsigned</span> <span class="n">num_rows_remaining</span> <span class="o">=</span> <span class="n">num_inputs</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_iterations</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">num_rows_this_iter</span> <span class="o">=</span>
                <span class="n">min2</span><span class="p">(</span><span class="n">num_rows_remaining</span><span class="p">,</span> <span class="n">num_rows_per_iteration</span><span class="p">);</span>
        <span class="c1">// We can ignore align_pad here because num_neurons has already</span>
        <span class="c1">// accounted for the original required padding.</span>
        <span class="c1">//</span>
        <span class="c1">// If this is not the last iteration, add one to the rows to fake a row</span>
        <span class="c1">// of biases (dimensionally). The bias will be skipped.</span>
        <span class="kt">bool</span> <span class="n">is_last_iter</span> <span class="o">=</span> <span class="n">i</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
        <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span>
                <span class="p">(</span><span class="n">dims_t</span><span class="p">){</span> <span class="n">num_rows_this_iter</span> <span class="o">+</span> <span class="p">(</span><span class="n">is_last_iter</span> <span class="o">?</span> <span class="mi">0</span> <span class="o">:</span> <span class="mi">1</span><span class="p">),</span>
                          <span class="n">num_neurons</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span> <span class="p">};</span>
        <span class="n">num_rows_remaining</span> <span class="o">-=</span> <span class="n">num_rows_this_iter</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">fc_cfgs</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// Call the right HW function based on the device parameters.</span>
<span class="kt">void</span> <span class="nf">smiv_inner_product_layer_hw_dispatch</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">activations</span><span class="p">,</span>
                                          <span class="kt">float</span><span class="o">*</span> <span class="n">weights</span><span class="p">,</span>
                                          <span class="kt">float</span><span class="o">*</span> <span class="n">results</span><span class="p">,</span>
                                          <span class="n">layer_t</span><span class="o">*</span> <span class="n">layer</span><span class="p">,</span>
                                          <span class="kt">int</span> <span class="n">result_size</span><span class="p">,</span>
                                          <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">,</span>
                                          <span class="c1">// Make a copy here.</span>
                                          <span class="n">smiv_inner_product_options</span> <span class="n">options</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">io_req_t</span> <span class="n">input_req</span> <span class="o">=</span> <span class="n">layer</span><span class="o">-&gt;</span><span class="n">input_req</span><span class="p">;</span>
    <span class="n">io_req_t</span> <span class="n">weights_req</span> <span class="o">=</span> <span class="n">layer</span><span class="o">-&gt;</span><span class="n">weights_req</span><span class="p">;</span>
    <span class="n">io_req_t</span> <span class="n">output_req</span> <span class="o">=</span> <span class="n">layer</span><span class="o">-&gt;</span><span class="n">output_req</span><span class="p">;</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">output_req</span> <span class="o">!=</span> <span class="n">IO_NONE</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">MAP_ARRAY_TO_ACCEL</span><span class="p">(</span><span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kInnerProductHw</span><span class="p">,</span>
                           <span class="n">get_host_results_var_name</span><span class="p">(</span><span class="n">output_req</span><span class="p">),</span>
                           <span class="n">results</span><span class="p">,</span>
                           <span class="n">result_size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="c1">// This needs to be handled separately from the inputs IO because if we</span>
    <span class="c1">// used compressed weights, then they have already been DMAed and</span>
    <span class="c1">// decompressed by the point we reach here.</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">weights_req</span> <span class="o">==</span> <span class="n">IO_DMA</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">assert</span><span class="p">(</span><span class="n">weights</span> <span class="o">&amp;&amp;</span> <span class="s">&quot;Cannot DMA weights if weights don&#39;t exist!&quot;</span><span class="p">);</span>
        <span class="n">begin_ignored_profiling</span><span class="p">(</span><span class="n">layer</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">);</span>
        <span class="kt">int</span> <span class="n">weights_size</span> <span class="o">=</span> <span class="n">get_num_weights_layer</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
        <span class="n">flush_cache_range</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">weights_size</span><span class="p">);</span>
        <span class="n">end_profiling</span><span class="p">();</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">input_req</span> <span class="o">==</span> <span class="n">IO_DMA</span> <span class="o">||</span> <span class="n">input_req</span> <span class="o">==</span> <span class="n">IO_NONE</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Use DMA for weights/activations.</span>
        <span class="c1">// Flush cache lines for activations and weights.</span>
        <span class="n">begin_ignored_profiling</span><span class="p">(</span><span class="n">layer</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">);</span>
        <span class="kt">int</span> <span class="n">activations_size</span> <span class="o">=</span>
                <span class="n">get_input_activations_size</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
        <span class="n">flush_cache_range</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">activations_size</span><span class="p">);</span>
        <span class="n">end_profiling</span><span class="p">();</span>
    <span class="p">}</span>

    <span class="c1">// This object is an internal structure only for the purposes of</span>
    <span class="c1">// simplifying the dispatch mechanism conditional checks!</span>
    <span class="n">access_config</span> <span class="n">access_config</span><span class="p">;</span>
    <span class="n">access_config</span><span class="p">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">io_to_access_mechanism</span><span class="p">(</span><span class="n">layer</span><span class="o">-&gt;</span><span class="n">input_req</span><span class="p">);</span>
    <span class="n">access_config</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">io_to_access_mechanism</span><span class="p">(</span><span class="n">layer</span><span class="o">-&gt;</span><span class="n">weights_req</span><span class="p">);</span>
    <span class="n">access_config</span><span class="p">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">io_to_access_mechanism</span><span class="p">(</span><span class="n">layer</span><span class="o">-&gt;</span><span class="n">output_req</span><span class="p">);</span>
    <span class="n">INVOKE_KERNEL_PROF</span><span class="p">(</span><span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kInnerProductHw</span><span class="p">,</span>
                       <span class="n">layer</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">,</span>
                       <span class="n">inner_product_layer_hw</span><span class="p">,</span>
                       <span class="c1">// DMA</span>
                       <span class="n">activations</span><span class="p">,</span>
                       <span class="n">weights</span><span class="p">,</span>
                       <span class="n">results</span><span class="p">,</span>
                       <span class="c1">// CACHE</span>
                       <span class="n">activations</span><span class="p">,</span>
                       <span class="n">weights</span><span class="p">,</span>
                       <span class="n">results</span><span class="p">,</span>
                       <span class="c1">// ACP</span>
                       <span class="n">activations</span><span class="p">,</span>
                       <span class="n">weights</span><span class="p">,</span>
                       <span class="n">results</span><span class="p">,</span>
                       <span class="c1">// Local scratchpads</span>
                       <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">umem</span><span class="p">,</span>
                       <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad0</span><span class="p">,</span>
                       <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad1</span><span class="p">,</span>
                       <span class="c1">// Other options</span>
                       <span class="n">layer</span><span class="p">,</span>
                       <span class="o">&amp;</span><span class="n">access_config</span><span class="p">,</span>
                       <span class="o">&amp;</span><span class="n">options</span><span class="p">);</span>

<span class="p">}</span>

<span class="kt">void</span> <span class="nf">smiv_inner_product_layer_impl_rowwise</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">host_activations</span><span class="p">,</span>
                                           <span class="kt">float</span><span class="o">*</span> <span class="n">host_weights</span><span class="p">,</span>
                                           <span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                           <span class="kt">float</span><span class="o">*</span> <span class="n">host_results</span><span class="p">,</span>
                                           <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">,</span>
                                           <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                                           <span class="kt">bool</span> <span class="n">input_in_spad0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">INFO_MSG</span><span class="p">(</span><span class="s">&quot;Running rowwise inner product.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="n">fc_cfg_t</span> <span class="n">fc_cfgs</span> <span class="o">=</span>
            <span class="n">smiv_inner_product_divide_work_rowwise</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">);</span>
    <span class="n">INFO_MSG</span><span class="p">(</span><span class="s">&quot;Inner product layer %d work configuration:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">);</span>
    <span class="n">print_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">);</span>
    <span class="kt">bool</span> <span class="n">needs_multiple_iter</span> <span class="o">=</span> <span class="p">(</span><span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">);</span>
    <span class="kt">bool</span> <span class="n">do_bias_in_software</span> <span class="o">=</span>
            <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="p">].</span><span class="n">rows</span> <span class="o">==</span> <span class="mi">1</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">do_bias_in_software</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// A NULL host_weights pointer is only passed if the weights are</span>
        <span class="c1">// compressed.</span>
        <span class="n">assert</span><span class="p">(</span><span class="n">host_weights</span> <span class="o">&amp;&amp;</span>
               <span class="s">&quot;Host weights cannot be NULL if bias is done in SW!&quot;</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="c1">// Holds a contiguous column of inputs and the partial results. If work</span>
    <span class="c1">// division is required, then each iteration&#39;s chunk of inputs is copied</span>
    <span class="c1">// into the buffer; otherwise, we just use the original input and results</span>
    <span class="c1">// buffers.</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">host_inputs_buffer</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">host_results_buffer</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">size_t</span> <span class="n">inputs_buffer_size</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span>
                                      <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">*</span>
                                      <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">host_inputs_buffer</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span><span class="n">inputs_buffer_size</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">host_inputs_buffer</span> <span class="o">=</span> <span class="n">host_activations</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">NUM_TEST_CASES</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">host_results_buffer</span> <span class="o">=</span>
                <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span><span class="n">get_dims_size</span><span class="p">(</span><span class="o">&amp;</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">)</span> <span class="o">*</span>
                                       <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span> <span class="o">*</span> <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">host_results_buffer</span> <span class="o">=</span> <span class="n">host_results</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="n">MAP_ARRAY_TO_ACCEL</span><span class="p">(</span><span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kInnerProductHw</span><span class="p">,</span>
                       <span class="n">get_host_inputs_var_name</span><span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span><span class="p">),</span>
                       <span class="n">host_inputs_buffer</span><span class="p">,</span>
                       <span class="n">inputs_buffer_size</span><span class="p">);</span>

    <span class="kt">int</span> <span class="n">current_row</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">current_result</span> <span class="o">=</span> <span class="n">host_results_buffer</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">curr_dense_weights_loc</span> <span class="o">=</span> <span class="n">host_weights</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">;</span> <span class="n">it</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">dims_t</span><span class="o">*</span> <span class="n">curr_iter</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="n">it</span><span class="p">];</span>
        <span class="kt">bool</span> <span class="n">is_last_iter</span> <span class="o">=</span> <span class="p">(</span><span class="n">it</span> <span class="o">==</span> <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>
        <span class="kt">bool</span> <span class="n">do_bias</span> <span class="o">=</span> <span class="n">is_last_iter</span> <span class="o">?</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">:</span> <span class="nb">false</span><span class="p">;</span>
        <span class="c1">// If the work division is such that at the end, we&#39;ve done all the</span>
        <span class="c1">// multiplicative weights, and we only need to do the bias now, then we</span>
        <span class="c1">// just run the bias on the CPU.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">is_last_iter</span> <span class="o">&amp;&amp;</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">break</span><span class="p">;</span>

        <span class="n">layer_t</span> <span class="n">partial_layer</span> <span class="o">=</span> <span class="o">*</span><span class="n">curr_layer</span><span class="p">;</span>
        <span class="n">partial_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="o">*</span><span class="n">curr_iter</span><span class="p">;</span>
        <span class="n">partial_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span><span class="p">;</span>
        <span class="n">partial_layer</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">(</span><span class="n">dims_t</span><span class="p">){</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="p">};</span>
        <span class="n">activation_type</span> <span class="n">act_func</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">activation</span><span class="p">;</span>
        <span class="kt">bool</span> <span class="n">do_hw_activation</span> <span class="o">=</span>
                <span class="n">device</span><span class="o">-&gt;</span><span class="n">use_hw_activation_func</span> <span class="o">&amp;&amp;</span>
                <span class="n">smiv_is_supported_activation_func</span><span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">,</span> <span class="n">act_func</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">do_hw_activation</span><span class="p">)</span>
            <span class="n">partial_layer</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">NO_ACTIVATION</span><span class="p">;</span>

        <span class="kt">int</span> <span class="n">iter_weights_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span>
                                <span class="p">(</span><span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">align_pad</span><span class="p">);</span>
        <span class="n">PRINT_MSG</span><span class="p">(</span><span class="s">&quot;FC iteration %d: weights %dx%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                   <span class="n">it</span><span class="p">,</span>
                   <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                   <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span><span class="p">);</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">copy_data_col_range</span><span class="p">(</span><span class="n">host_activations</span><span class="p">,</span>
                                <span class="o">&amp;</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">,</span>
                                <span class="n">current_row</span><span class="p">,</span>
                                <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                                <span class="n">host_inputs_buffer</span><span class="p">);</span>

            <span class="n">PRINT_MSG_V</span><span class="p">(</span><span class="s">&quot;inputs buffer</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
            <span class="n">PRINT_DEBUG_V</span><span class="p">(</span><span class="n">host_inputs_buffer</span><span class="p">,</span>
                          <span class="n">partial_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span> <span class="n">NUM_TEST_CASES</span><span class="p">,</span>
                          <span class="n">partial_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">cols</span><span class="p">,</span>
                          <span class="n">partial_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span>
                                  <span class="n">partial_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">align_pad</span><span class="p">);</span>
            <span class="n">partial_layer</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">NO_ACTIVATION</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c1">// First decompress the weights.</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">host_weights</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">PackedCSR</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// If this is not the last iteration, then pre-emptively subtract</span>
            <span class="c1">// one from the rows to get rid of decompressing an extra row for</span>
            <span class="c1">// nothing.</span>
            <span class="n">layer_t</span> <span class="n">temp_layer</span> <span class="o">=</span> <span class="n">partial_layer</span><span class="p">;</span>
            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">is_last_iter</span><span class="p">)</span>
                <span class="n">temp_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="o">--</span><span class="p">;</span>
            <span class="n">smiv_decompress_packed_csr_impl</span><span class="p">(</span><span class="o">&amp;</span><span class="n">temp_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">current_row</span><span class="p">,</span>
                                            <span class="n">input_in_spad0</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>
            <span class="c1">// Now that we&#39;ve decompressed the weights, we don&#39;t need to DMA</span>
            <span class="c1">// them again.</span>
            <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights_req</span> <span class="o">=</span> <span class="n">IO_NONE</span><span class="p">;</span>
            <span class="c1">// Make sure we don&#39;t try to access host weights, since the</span>
            <span class="c1">// decompressed weights don&#39;t exist on the host.</span>
            <span class="n">curr_dense_weights_loc</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
            <span class="n">PRINT_MSG</span><span class="p">(</span><span class="s">&quot;Weights:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
            <span class="n">PRINT_DEBUG</span><span class="p">(</span><span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">umem</span><span class="p">,</span>
                        <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                        <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span><span class="p">,</span>
                        <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span>
                                <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">partial_layer</span><span class="p">.</span><span class="n">weights_req</span> <span class="o">!=</span> <span class="n">IO_NONE</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">const</span> <span class="kt">size_t</span> <span class="n">weights_buffer_size</span> <span class="o">=</span>
                    <span class="p">(</span><span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">align_pad</span><span class="p">)</span> <span class="o">*</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span> <span class="o">*</span>
                    <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
            <span class="n">MAP_ARRAY_TO_ACCEL</span><span class="p">(</span>
                    <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kInnerProductHw</span><span class="p">,</span>
                    <span class="n">get_host_weights_var_name</span><span class="p">(</span><span class="n">partial_layer</span><span class="p">.</span><span class="n">weights_req</span><span class="p">),</span>
                    <span class="n">curr_dense_weights_loc</span><span class="p">,</span>
                    <span class="n">weights_buffer_size</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="kt">size_t</span> <span class="n">result_size</span> <span class="o">=</span> <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">partial_layer</span><span class="p">.</span><span class="n">inputs</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span>
                             <span class="n">partial_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span><span class="p">;</span>

        <span class="n">smiv_inner_product_options</span> <span class="n">options</span><span class="p">;</span>
        <span class="n">options</span><span class="p">.</span><span class="n">do_bias</span> <span class="o">=</span> <span class="n">do_bias</span><span class="p">;</span>
        <span class="n">options</span><span class="p">.</span><span class="n">input_in_spad0</span> <span class="o">=</span> <span class="n">input_in_spad0</span><span class="p">;</span>
        <span class="n">options</span><span class="p">.</span><span class="n">use_pipelined_dma</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">use_pipelined_dma</span><span class="p">;</span>
        <span class="n">smiv_inner_product_layer_hw_dispatch</span><span class="p">(</span><span class="n">host_inputs_buffer</span><span class="p">,</span>
                                             <span class="n">curr_dense_weights_loc</span><span class="p">,</span>
                                             <span class="n">current_result</span><span class="p">,</span>
                                             <span class="o">&amp;</span><span class="n">partial_layer</span><span class="p">,</span>
                                             <span class="n">result_size</span><span class="p">,</span>
                                             <span class="n">g_smiv</span><span class="p">,</span>
                                             <span class="n">options</span><span class="p">);</span>

        <span class="n">PRINT_MSG_V</span><span class="p">(</span><span class="s">&quot;Partial results:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG_V</span><span class="p">(</span><span class="n">current_result</span><span class="p">,</span>
                      <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">inputs</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span> <span class="n">NUM_TEST_CASES</span><span class="p">,</span>
                      <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span><span class="p">,</span>
                      <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span><span class="p">);</span>

        <span class="n">current_row</span> <span class="o">+=</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
        <span class="n">current_result</span> <span class="o">+=</span> <span class="n">result_size</span><span class="p">;</span>
        <span class="n">curr_dense_weights_loc</span> <span class="o">+=</span> <span class="n">iter_weights_size</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// If multiple iterations were needed, do a final round of reduction on the</span>
    <span class="c1">// partial sums.</span>
    <span class="c1">//</span>
    <span class="c1">// We have ITER blocks of NxM partial sums, where NxM is the final output</span>
    <span class="c1">// dimensions. Accumulate elementwise.</span>
    <span class="c1">//</span>
    <span class="c1">// TODO: For now, do on the CPU - but maybe using the reduction HW is</span>
    <span class="c1">// worthwhile?</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// SMAUG expects this function to run the activation function if the HW</span>
        <span class="c1">// supports it and the user has not specified use_hw_activation_func =</span>
        <span class="c1">// false. But this particular flavor of dividing the inputs means that</span>
        <span class="c1">// unless we reduce in HW, we can&#39;t run activation functions in</span>
        <span class="c1">// hardware, since we never have the fully reduced sum there. As a</span>
        <span class="c1">// result, we have to run the activation function here, before we</span>
        <span class="c1">// return.</span>
        <span class="n">activation_type</span> <span class="n">act_func</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">activation</span><span class="p">;</span>
        <span class="kt">bool</span> <span class="n">do_activation</span> <span class="o">=</span> <span class="n">act_func</span> <span class="o">!=</span> <span class="n">NO_ACTIVATION</span><span class="p">;</span>
        <span class="kt">bool</span> <span class="n">do_hw_activation</span> <span class="o">=</span>
                <span class="n">device</span><span class="o">-&gt;</span><span class="n">use_hw_activation_func</span> <span class="o">&amp;&amp;</span>
                <span class="n">smiv_is_supported_activation_func</span><span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">,</span> <span class="n">act_func</span><span class="p">);</span>
        <span class="kt">bool</span> <span class="n">do_activation_here</span> <span class="o">=</span> <span class="n">do_activation</span> <span class="o">&amp;&amp;</span> <span class="n">do_hw_activation</span><span class="p">;</span>

        <span class="kt">int</span> <span class="n">output_rows</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">.</span><span class="n">rows</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">output_cols</span> <span class="o">=</span>
                <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">.</span><span class="n">align_pad</span><span class="p">;</span>
        <span class="n">ARRAY_3D</span><span class="p">(</span><span class="kt">float</span><span class="p">,</span>
                 <span class="n">_temp_results</span><span class="p">,</span>
                 <span class="n">host_results_buffer</span><span class="p">,</span>
                 <span class="n">output_rows</span><span class="p">,</span>
                 <span class="n">output_cols</span><span class="p">);</span>  <span class="c1">// temp buffer</span>
        <span class="n">ARRAY_2D</span><span class="p">(</span><span class="kt">float</span><span class="p">,</span>
                 <span class="n">_host_results</span><span class="p">,</span>
                 <span class="n">host_results</span><span class="p">,</span>
                 <span class="n">output_cols</span><span class="p">);</span>                 <span class="c1">// dst buffer.</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">biases</span> <span class="o">=</span> <span class="n">host_weights</span> <span class="o">+</span>
                        <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span><span class="p">);</span>
        <span class="n">current_result</span> <span class="o">=</span> <span class="n">host_results_buffer</span><span class="p">;</span>  <span class="c1">// temporary buffer.</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">r</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">r</span> <span class="o">&lt;</span> <span class="n">output_rows</span><span class="p">;</span> <span class="n">r</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">output_cols</span><span class="p">;</span> <span class="n">c</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="kt">float</span> <span class="n">accum</span> <span class="o">=</span> <span class="n">do_bias_in_software</span> <span class="o">?</span> <span class="n">biases</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
                <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">;</span> <span class="n">it</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                  <span class="n">accum</span> <span class="o">+=</span> <span class="n">_temp_results</span><span class="p">[</span><span class="n">it</span><span class="p">][</span><span class="n">r</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
                <span class="p">}</span>

                <span class="c1">// If we need to do the activation function here, write it</span>
                <span class="c1">// straight back to temp_results, so we can use host_results</span>
                <span class="c1">// for the activation function results array.</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">do_activation_here</span><span class="p">)</span>
                  <span class="n">_temp_results</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">r</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">accum</span><span class="p">;</span>
                <span class="k">else</span>
                  <span class="n">_host_results</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">accum</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">do_activation_here</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// TODO: This means it will be harder to separate the MKL primitive</span>
            <span class="c1">// construction time from the actual activation function runtime.</span>
            <span class="n">smiv_activation_function_impl</span><span class="p">(</span>
                    <span class="n">host_results_buffer</span><span class="p">,</span> <span class="n">curr_layer</span><span class="p">,</span> <span class="n">host_results</span><span class="p">,</span> <span class="n">device</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">free</span><span class="p">(</span><span class="n">host_inputs_buffer</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">NUM_TEST_CASES</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">free</span><span class="p">(</span><span class="n">host_results_buffer</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">free_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">smiv_inner_product_layer_impl_colwise</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">host_activations</span><span class="p">,</span>
                                           <span class="kt">float</span><span class="o">*</span> <span class="n">host_weights</span><span class="p">,</span>
                                           <span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span><span class="p">,</span>
                                           <span class="kt">float</span><span class="o">*</span> <span class="n">host_results</span><span class="p">,</span>
                                           <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">,</span>
                                           <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">,</span>
                                           <span class="kt">bool</span> <span class="n">input_in_spad0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">INFO_MSG</span><span class="p">(</span><span class="s">&quot;Running colwise inner product.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
    <span class="n">fc_cfg_t</span> <span class="n">fc_cfgs</span> <span class="o">=</span>
            <span class="n">smiv_inner_product_divide_work_colwise</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="n">g_smiv</span><span class="p">);</span>
    <span class="n">INFO_MSG</span><span class="p">(</span><span class="s">&quot;Inner product layer %d work configuration:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">num</span><span class="p">);</span>
    <span class="n">print_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">);</span>

    <span class="kt">bool</span> <span class="n">needs_multiple_iter</span> <span class="o">=</span> <span class="p">(</span><span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">);</span>
    <span class="n">MAP_ARRAY_TO_ACCEL</span><span class="p">(</span><span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kInnerProductHw</span><span class="p">,</span>
                       <span class="n">get_host_inputs_var_name</span><span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">input_req</span><span class="p">),</span>
                       <span class="n">host_activations</span><span class="p">,</span>
                       <span class="n">INPUT_BYTES</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">));</span>

    <span class="c1">// Holds a contiguous column of weights and the partial results. If work</span>
    <span class="c1">// division is required, then each iteration&#39;s chunk of weights is copied</span>
    <span class="c1">// into the buffer; otherwise, we just use the original weights and results</span>
    <span class="c1">// buffers.</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">host_weights_buffer</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">host_results_buffer</span><span class="p">;</span>
    <span class="k">const</span> <span class="kt">size_t</span> <span class="n">weights_buffer_size</span> <span class="o">=</span>
            <span class="p">(</span><span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">cols</span> <span class="o">+</span> <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">align_pad</span><span class="p">)</span> <span class="o">*</span>
            <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">rows</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">host_weights_buffer</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span><span class="n">weights_buffer_size</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">host_weights_buffer</span> <span class="o">=</span> <span class="n">host_weights</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">NUM_TEST_CASES</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">host_results_buffer</span> <span class="o">=</span>
                <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">malloc_aligned</span><span class="p">(</span><span class="n">OUTPUT_BYTES</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">));</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="n">host_results_buffer</span> <span class="o">=</span> <span class="n">host_results</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="kt">int</span> <span class="n">current_col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">current_result</span> <span class="o">=</span> <span class="n">host_results_buffer</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">;</span> <span class="n">it</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">layer_t</span> <span class="n">partial_layer</span> <span class="o">=</span> <span class="o">*</span><span class="n">curr_layer</span><span class="p">;</span>
        <span class="n">dims_t</span><span class="o">*</span> <span class="n">curr_iter</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="n">it</span><span class="p">];</span>
        <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span> <span class="o">=</span> <span class="o">*</span><span class="n">curr_iter</span><span class="p">;</span>
        <span class="n">partial_layer</span><span class="p">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">(</span><span class="n">dims_t</span><span class="p">){</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="p">};</span>
        <span class="n">partial_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">=</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span><span class="p">;</span>

        <span class="n">activation_type</span> <span class="n">act_func</span> <span class="o">=</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">activation</span><span class="p">;</span>
        <span class="kt">bool</span> <span class="n">do_hw_activation</span> <span class="o">=</span>
                <span class="n">device</span><span class="o">-&gt;</span><span class="n">use_hw_activation_func</span> <span class="o">&amp;&amp;</span>
                <span class="n">smiv_is_supported_activation_func</span><span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">,</span> <span class="n">act_func</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">do_hw_activation</span><span class="p">)</span>
            <span class="n">partial_layer</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">NO_ACTIVATION</span><span class="p">;</span>

        <span class="n">PRINT_MSG</span><span class="p">(</span><span class="s">&quot;FC iteration %d: weights %dx%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span>
                  <span class="n">it</span><span class="p">,</span>
                  <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                  <span class="n">partial_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span><span class="p">);</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">copy_data_col_range</span><span class="p">(</span><span class="n">host_weights</span><span class="p">,</span>
                                <span class="o">&amp;</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">,</span>
                                <span class="n">current_col</span><span class="p">,</span>
                                <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">align_pad</span><span class="p">,</span>
                                <span class="n">host_weights_buffer</span><span class="p">);</span>

            <span class="n">PRINT_DEBUG_V</span><span class="p">(</span><span class="n">host_weights_buffer</span><span class="p">,</span>
                          <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">rows</span><span class="p">,</span>
                          <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span><span class="p">,</span>
                          <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span> <span class="o">+</span>
                                  <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">align_pad</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights_req</span> <span class="o">!=</span> <span class="n">IO_NONE</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">MAP_ARRAY_TO_ACCEL</span><span class="p">(</span>
                    <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">kInnerProductHw</span><span class="p">,</span>
                    <span class="n">get_host_weights_var_name</span><span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights_req</span><span class="p">),</span>
                    <span class="n">host_weights_buffer</span><span class="p">,</span>
                    <span class="n">weights_buffer_size</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="kt">size_t</span> <span class="n">result_size</span> <span class="o">=</span> <span class="n">NUM_TEST_CASES</span> <span class="o">*</span> <span class="n">partial_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">rows</span> <span class="o">*</span>
                             <span class="n">partial_layer</span><span class="p">.</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span><span class="p">;</span>

        <span class="n">smiv_inner_product_options</span> <span class="n">options</span><span class="p">;</span>
        <span class="n">options</span><span class="p">.</span><span class="n">do_bias</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
        <span class="n">options</span><span class="p">.</span><span class="n">input_in_spad0</span> <span class="o">=</span> <span class="n">input_in_spad0</span><span class="p">;</span>
        <span class="n">options</span><span class="p">.</span><span class="n">use_pipelined_dma</span> <span class="o">=</span> <span class="n">device</span><span class="o">-&gt;</span><span class="n">use_pipelined_dma</span><span class="p">;</span>
        <span class="n">smiv_inner_product_layer_hw_dispatch</span><span class="p">(</span><span class="n">host_activations</span><span class="p">,</span>
                                             <span class="n">host_weights_buffer</span><span class="p">,</span>
                                             <span class="n">current_result</span><span class="p">,</span>
                                             <span class="o">&amp;</span><span class="n">partial_layer</span><span class="p">,</span>
                                             <span class="n">result_size</span><span class="p">,</span>
                                             <span class="n">g_smiv</span><span class="p">,</span>
                                             <span class="n">options</span><span class="p">);</span>

        <span class="n">PRINT_MSG_V</span><span class="p">(</span><span class="s">&quot;Partial results:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG_V</span><span class="p">(</span>
                <span class="n">current_result</span><span class="p">,</span>
                <span class="n">NUM_TEST_CASES</span><span class="p">,</span>
                <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span><span class="p">,</span>
                <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">align_pad</span><span class="p">);</span>

        <span class="n">current_col</span> <span class="o">+=</span> <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span><span class="p">;</span>
        <span class="n">current_result</span> <span class="o">+=</span> <span class="n">result_size</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// Fix up the results if required.</span>
    <span class="c1">//</span>
    <span class="c1">// The desired result looks like (for batch size 2):</span>
    <span class="c1">//</span>
    <span class="c1">// [ input 1, iter 1 results ] [ input 1, iter 2 results ] ...</span>
    <span class="c1">// [ input 1, iter 1 results ] [ input 1, iter 2 results ] ...</span>
    <span class="c1">//</span>
    <span class="c1">// But, when the batch size &gt; 1 and multiple iterations are needed, the</span>
    <span class="c1">// results buffer will end up looking like this:</span>
    <span class="c1">//</span>
    <span class="c1">// [ input 1, iter 1 results ] [ input 2, iter 1 results ] ...</span>
    <span class="c1">// [ input 1, iter 2 results ] [ input 2, iter 2 results ] ...</span>
    <span class="c1">//</span>
    <span class="c1">// This routine reorders the results buffer and stores the result into the</span>
    <span class="c1">// final result array (host_results).</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">NUM_TEST_CASES</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">output_size</span> <span class="o">=</span>
                <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">outputs</span><span class="p">.</span><span class="n">align_pad</span><span class="p">;</span>
        <span class="n">ARRAY_2D</span><span class="p">(</span><span class="kt">float</span><span class="p">,</span> <span class="n">_host_results</span><span class="p">,</span> <span class="n">host_results</span><span class="p">,</span> <span class="n">output_size</span><span class="p">);</span>  <span class="c1">// dst buffer.</span>
        <span class="n">current_result</span> <span class="o">=</span> <span class="n">host_results_buffer</span><span class="p">;</span>  <span class="c1">// temporary buffer.</span>
        <span class="kt">int</span> <span class="n">curr_col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="n">it</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">it</span> <span class="o">&lt;</span> <span class="n">fc_cfgs</span><span class="p">.</span><span class="n">num_iterations</span><span class="p">;</span> <span class="n">it</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">dims_t</span><span class="o">*</span> <span class="n">curr_iter</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">.</span><span class="n">iteration</span><span class="p">[</span><span class="n">it</span><span class="p">];</span>
            <span class="kt">int</span> <span class="n">it_output_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">cols</span> <span class="o">+</span>
                                  <span class="n">curr_iter</span><span class="o">-&gt;</span><span class="n">align_pad</span><span class="p">);</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">tc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">tc</span> <span class="o">&lt;</span> <span class="n">NUM_TEST_CASES</span><span class="p">;</span> <span class="n">tc</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="n">memcpy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">_host_results</span><span class="p">[</span><span class="n">tc</span><span class="p">][</span><span class="n">curr_col</span><span class="p">],</span>
                       <span class="n">current_result</span><span class="p">,</span>
                       <span class="n">it_output_size</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">));</span>
                <span class="n">current_result</span> <span class="o">+=</span> <span class="n">it_output_size</span><span class="p">;</span>
            <span class="p">}</span>
            <span class="n">curr_col</span> <span class="o">+=</span> <span class="n">it_output_size</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">free</span><span class="p">(</span><span class="n">host_weights_buffer</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">NUM_TEST_CASES</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">needs_multiple_iter</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">free</span><span class="p">(</span><span class="n">host_results_buffer</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="n">free_smiv_work_cfg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fc_cfgs</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">smiv_inner_product_layer_impl</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">host_activations</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">host_weights</span><span class="p">,</span>
                                   <span class="n">layer_t</span><span class="o">*</span> <span class="n">layers</span><span class="p">,</span>
                                   <span class="kt">int</span> <span class="n">lnum</span><span class="p">,</span>
                                   <span class="kt">float</span><span class="o">*</span> <span class="n">host_results</span><span class="p">,</span>
                                   <span class="n">smiv_global</span><span class="o">*</span> <span class="n">g_smiv</span><span class="p">,</span>
                                   <span class="n">device_t</span><span class="o">*</span> <span class="n">device</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">static</span> <span class="kt">float</span><span class="o">*</span> <span class="n">current_result_loc</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">current_result_loc</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">current_result_loc</span> <span class="o">=</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad1</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">current_result_loc</span> <span class="o">==</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">current_result_loc</span> <span class="o">=</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad1</span><span class="p">;</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">current_result_loc</span> <span class="o">==</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">current_result_loc</span> <span class="o">=</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad0</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="kt">bool</span> <span class="n">input_in_spad0</span> <span class="o">=</span> <span class="p">(</span><span class="n">current_result_loc</span> <span class="o">==</span> <span class="n">g_smiv</span><span class="o">-&gt;</span><span class="n">spad1</span><span class="p">);</span>
    <span class="n">layer_t</span><span class="o">*</span> <span class="n">curr_layer</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">layers</span><span class="p">[</span><span class="n">lnum</span><span class="p">];</span>
    <span class="n">assert</span><span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">host_weights</span><span class="o">-&gt;</span><span class="n">len</span> <span class="o">==</span> <span class="mi">1</span> <span class="o">&amp;&amp;</span>
           <span class="s">&quot;SMIV only requires one set of weights!&quot;</span><span class="p">);</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">host_weights</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">Uncompressed</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">float</span><span class="o">*</span> <span class="n">host_weights_layer</span> <span class="o">=</span>
                <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">host_weights</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">dense</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">;</span>
        <span class="n">PRINT_MSG</span><span class="p">(</span><span class="s">&quot;Weights:</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">PRINT_DEBUG</span><span class="p">(</span><span class="n">host_weights_layer</span><span class="p">,</span>
                    <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">rows</span><span class="p">,</span>
                    <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span><span class="p">,</span>
                    <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">cols</span> <span class="o">+</span> <span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">weights</span><span class="p">.</span><span class="n">align_pad</span><span class="p">);</span>

        <span class="c1">// Dynamically pick one to use, based on whether weights or inputs are</span>
        <span class="c1">// better. If inputs is bigger, we&#39;ll divide the work columnwise in the</span>
        <span class="c1">// weights and reorder the weights instead of the inputs, which are</span>
        <span class="c1">// larger. But if weights are bigger, we&#39;ll divide the work rowwise in</span>
        <span class="c1">// the weights. This means we can just pass a pointer to the current</span>
        <span class="c1">// location in the weights and only reorder the inputs (the smaller</span>
        <span class="c1">// input to the GEMM).</span>
        <span class="kt">int</span> <span class="n">input_size</span> <span class="o">=</span> <span class="n">get_input_activations_size</span><span class="p">(</span><span class="n">curr_layer</span><span class="p">);</span>
        <span class="kt">int</span> <span class="n">weight_size</span> <span class="o">=</span> <span class="n">get_num_weights_layer</span><span class="p">(</span><span class="n">layers</span><span class="p">,</span> <span class="n">lnum</span><span class="p">);</span>
        <span class="n">INFO_MSG</span><span class="p">(</span><span class="s">&quot;Input size: %d, weight size: %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">weight_size</span><span class="p">);</span>
        <span class="n">tiled_inner_product_impl</span> <span class="n">impl</span> <span class="o">=</span>
                <span class="p">(</span><span class="n">input_size</span> <span class="o">&gt;</span> <span class="n">weight_size</span><span class="p">)</span>
                        <span class="o">?</span> <span class="o">&amp;</span><span class="nl">smiv_inner_product_layer_impl_colwise</span>
                        <span class="p">:</span> <span class="o">&amp;</span><span class="n">smiv_inner_product_layer_impl_rowwise</span><span class="p">;</span>
        <span class="n">impl</span><span class="p">(</span><span class="n">host_activations</span><span class="p">,</span> <span class="n">host_weights_layer</span><span class="p">,</span> <span class="n">curr_layer</span><span class="p">,</span> <span class="n">host_results</span><span class="p">,</span>
             <span class="n">g_smiv</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">input_in_spad0</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">host_weights</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">PackedCSR</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// If the weights are stored in CSR format, we can only do row-wise</span>
        <span class="c1">// tiling.</span>
        <span class="n">INFO_MSG</span><span class="p">(</span><span class="s">&quot;Running rowwise inner product for packed CSR weights.</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">smiv_inner_product_layer_impl_rowwise</span><span class="p">(</span><span class="n">host_activations</span><span class="p">,</span>
                                              <span class="nb">NULL</span><span class="p">,</span>
                                              <span class="n">curr_layer</span><span class="p">,</span>
                                              <span class="n">host_results</span><span class="p">,</span>
                                              <span class="n">g_smiv</span><span class="p">,</span>
                                              <span class="n">device</span><span class="p">,</span>
                                              <span class="n">input_in_spad0</span><span class="p">);</span>
    <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="p">(</span><span class="n">curr_layer</span><span class="o">-&gt;</span><span class="n">host_weights</span><span class="o">-&gt;</span><span class="n">type</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">CSR</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">fprintf</span><span class="p">(</span><span class="n">stderr</span><span class="p">,</span> <span class="s">&quot;Inner product layer for unpacked CSR weights is not &quot;</span>
                        <span class="s">&quot;supported!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
        <span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, SMAUG Contributors

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>